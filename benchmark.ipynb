{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from nltk.translate import bleu_score\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "\n",
    "from metrics.Bary_score import BaryScoreMetric\n",
    "from metrics.InfoLM import InfoLM\n",
    "from metrics.Depth_score import DepthScoreMetric\n",
    "from metrics.WBleu import WeightedEntropyBLEU\n",
    "\n",
    "    \n",
    "from rouge import Rouge\n",
    "from sacrebleu import corpus_bleu\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from moverscore import get_idf_dict, word_mover_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation des métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(dataset, gen):\n",
    "    \n",
    "    if gen not in ['gpt', 'plan_write', 's2s', 'gpt_kg', 'fusion']:\n",
    "        raise ValueError(\"Le générateur doit être 'gpt', 'plan_write', 's2s', 'gpt_kg' ou 'fusion'.\")\n",
    "    \n",
    "    # Initialisation du corpus de référence pour la métrique WeightedEntropyBLEU\n",
    "    reference_corpus = [data['gold_response'] for id, data in dataset.items()]\n",
    "    \n",
    "    # Initialisation des instances calculant les métriques\n",
    "    weighted_entropy_bleu = WeightedEntropyBLEU(reference_corpus)\n",
    "    rouge = Rouge()\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "    bary_score_metric = BaryScoreMetric(model_name ,use_idfs=False)\n",
    "    info_lm_metric = InfoLM(model_name, measure_to_use='kl', alpha=0.25, beta=0.25, temperature=1, use_idf_weights=False)\n",
    "    depth_score_metric = DepthScoreMetric(model_name, layers_to_consider=4)\n",
    "\n",
    "    # Initialisation des listes de stockages de calcul des métriques\n",
    "    bleu_scores = []\n",
    "    weighted_bleu_scores = []\n",
    "    rouge_1_r = []\n",
    "    rouge_1_p = []\n",
    "    rouge_1_f = []\n",
    "    rouge_2_r = []\n",
    "    rouge_2_p = []\n",
    "    rouge_2_f = []\n",
    "    rouge_l_r = []\n",
    "    rouge_l_p = []\n",
    "    rouge_l_f = []\n",
    "    bertscore_p = []\n",
    "    bertscore_r = []\n",
    "    bertscore_f1 = []\n",
    "    # moverscores = []\n",
    "    bary_scores_W = []\n",
    "    bary_scores_SD_10 = []\n",
    "    bary_scores_SD_5 = []\n",
    "    bary_scores_SD_1 = []\n",
    "    bary_scores_SD_0_5 = []\n",
    "    bary_scores_SD_0_1 = []\n",
    "    bary_scores_SD_0_01 = []\n",
    "    bary_scores_SD_0_001 = []\n",
    "    info_lm_kl = []\n",
    "    info_lm_r_kl = []\n",
    "    info_lm_sim_kl = []\n",
    "    depth_scores = []\n",
    "    human_scores = []\n",
    "    \n",
    "    \n",
    "\n",
    "    # Itération sur le dataset et mise en place des métriques de comparaison entre les réponses aux prompt de générateurs par rapport à la gold réponse\n",
    "    for id, data in dataset.items():\n",
    "        gold_response = data['gold_response']\n",
    "        response = data['gen'][gen]['text']\n",
    "        human_score = np.mean(data['gen'][gen]['score'])\n",
    "        human_scores.append(human_score)\n",
    "        \n",
    "        # Tokenisation des réponses\n",
    "        t_gold_response = bary_score_metric.tokenizer(gold_response, return_tensors='pt')['input_ids']\n",
    "        t_response = bary_score_metric.tokenizer(response, return_tensors='pt')['input_ids']\n",
    "\n",
    "        # Convertion token IDs en strings\n",
    "        t_gold_response_str = ' '.join(bary_score_metric.tokenizer.convert_ids_to_tokens(t_gold_response[0]))\n",
    "        t_response_str = ' '.join(bary_score_metric.tokenizer.convert_ids_to_tokens(t_response[0]))\n",
    "        \n",
    "        # Tokenisation des phrases en listes de mots pour computer BLEU\n",
    "        hyp_tokens = word_tokenize(t_response_str)\n",
    "        ref_tokens = word_tokenize(t_gold_response_str)\n",
    "        \n",
    "        # Calcul des différents scores de métriques\n",
    "        bleu_score = sentence_bleu([ref_tokens], hyp_tokens)\n",
    "        bleu_scores.append(bleu_score)\n",
    "        \n",
    "        entropy_bleu_score = weighted_entropy_bleu.score([ref_tokens], hyp_tokens)\n",
    "        weighted_bleu_scores.append(entropy_bleu_score)\n",
    "\n",
    "        rouge_scores = rouge.get_scores(t_response_str, t_gold_response_str, avg=True)\n",
    "        rouge_1_r.append(rouge_scores['rouge-1']['r'])\n",
    "        rouge_1_p.append(rouge_scores['rouge-1']['p'])\n",
    "        rouge_1_f.append(rouge_scores['rouge-1']['f'])\n",
    "        rouge_2_r.append(rouge_scores['rouge-2']['r'])\n",
    "        rouge_2_p.append(rouge_scores['rouge-2']['p'])\n",
    "        rouge_2_f.append(rouge_scores['rouge-2']['f'])\n",
    "        rouge_l_r.append(rouge_scores['rouge-l']['r'])\n",
    "        rouge_l_p.append(rouge_scores['rouge-l']['p'])\n",
    "        rouge_l_f.append(rouge_scores['rouge-l']['f'])\n",
    "\n",
    "        \n",
    "        \n",
    "        bary_score_metric.prepare_idfs([t_gold_response_str], [t_response_str])\n",
    "        bary_score = bary_score_metric.evaluate_batch([t_gold_response_str], [t_response_str])\n",
    "        info_lm_metric.prepare_idfs([t_gold_response_str], [t_response_str])\n",
    "        info_lm_score = info_lm_metric.evaluate_batch([t_gold_response_str], [t_response_str])\n",
    "        depth_score = depth_score_metric.evaluate_batch([t_gold_response_str], [t_response_str])\n",
    "\n",
    "        bary_scores_W.append(bary_score['baryscore_W'][0])\n",
    "        bary_scores_SD_10.append(bary_score['baryscore_SD_10'][0])\n",
    "        bary_scores_SD_5.append(bary_score['baryscore_SD_5'][0])\n",
    "        bary_scores_SD_1.append(bary_score['baryscore_SD_1'][0])\n",
    "        bary_scores_SD_0_5.append(bary_score['baryscore_SD_0.5'][0])\n",
    "        bary_scores_SD_0_1.append(bary_score['baryscore_SD_0.1'][0])\n",
    "        bary_scores_SD_0_01.append(bary_score['baryscore_SD_0.01'][0])\n",
    "        bary_scores_SD_0_001.append(bary_score['baryscore_SD_0.001'][0])\n",
    "        info_lm_kl.append(info_lm_score['kl'][0])\n",
    "        info_lm_r_kl.append(info_lm_score['r_kl'][0])\n",
    "        info_lm_sim_kl.append(info_lm_score['sim_kl'][0])\n",
    "        depth_scores.append(depth_score['depth_score'][0])\n",
    "        \n",
    "        P, R, F1 = bert_score([t_response_str], [t_gold_response_str], lang=\"en\", model_type=model_name)\n",
    "        bertscore_p.append(P.numpy()[0])\n",
    "        bertscore_r.append(R.numpy()[0])\n",
    "        bertscore_f1.append(F1.numpy()[0])\n",
    "        \n",
    "        # moverscore = word_mover_score([t_gold_response_str], [t_response_str], idf_dict_ref, stop_words=[], n_gram=1, remove_subwords=True)\n",
    "        # moverscores.append(moverscore[0])\n",
    "    \n",
    "\n",
    "\n",
    "    dict = {\n",
    "        'BLEU': bleu_scores,\n",
    "        'Weighted_BLEU': weighted_bleu_scores,\n",
    "        'Rouge_1_r': rouge_1_r,\n",
    "        'Rouge_1_p': rouge_1_p,\n",
    "        'Rouge_1_f': rouge_1_f,\n",
    "        'Rouge_2_r': rouge_2_r,\n",
    "        'Rouge_2_p': rouge_2_p,\n",
    "        'Rouge_2_f': rouge_2_f,\n",
    "        'Rouge_L_r': rouge_l_r,\n",
    "        'Rouge_L_p': rouge_l_p,\n",
    "        'Rouge_L_f': rouge_l_f,\n",
    "        'Bary_W': bary_scores_W,\n",
    "        'Bary_SD_10': bary_scores_SD_10,\n",
    "        'Bary_SD_5': bary_scores_SD_5,\n",
    "        'Bary_SD_1': bary_scores_SD_1,\n",
    "        'Bary_SD_0_5': bary_scores_SD_0_5,\n",
    "        'Bary_SD_0_1': bary_scores_SD_0_1,\n",
    "        'Bary_SD_0_01': bary_scores_SD_0_01,\n",
    "        'Bary_SD_0_001': bary_scores_SD_0_001,\n",
    "        'InfoLM_kl': info_lm_kl,\n",
    "        'InfoLM_r_kl': info_lm_r_kl,\n",
    "        'InfoLM_sim_kl': info_lm_sim_kl,\n",
    "        'Depth_scores': depth_scores,\n",
    "        'Bertscore_p': bertscore_p,\n",
    "        'Bertscore_r': bertscore_r,\n",
    "        'Bertscore_f1': bertscore_f1,\n",
    "        # 'moverscores': moverscores,\n",
    "        'human_scores': human_scores\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(dict)\n",
    "\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# subset_size = 10\n",
    "# subset_items = list(dataset.items())[:subset_size]\n",
    "\n",
    "# # créer un dictionnaire à partir des paires clé-valeur\n",
    "# subset = dict(subset_items)\n",
    "\n",
    "# df = compute_metrics(subset, gen='gpt')\n",
    "# print(df.corr())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation des correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(df, method='pearson', dataset_name='', gen='', save_path=None):\n",
    "    if method not in ['pearson', 'spearman', 'kendall']:\n",
    "        raise ValueError(\"La méthode doit être 'pearson', 'spearman' ou 'kendall'.\")\n",
    "\n",
    "    human_corr = df.corr(method=method)[\"human_scores\"].drop(\"human_scores\")\n",
    "    \n",
    "    plt.figure(figsize=(20, 1))\n",
    "    sns.heatmap(human_corr.to_frame().T, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, cbar=False)\n",
    "\n",
    "    method_capitalized = method.capitalize()\n",
    "    plt.title(f\"Correlations with human_score ({method_capitalized}) - Dataset: {dataset_name} - Gen: {gen}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.subplots_adjust(top=0.9, bottom=0.5)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        print(f\"Graphique enregistré sous : {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return human_corr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Itération des computations sur les deux dataset et pour les différents générateurs de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: WP\n",
      "Generation: gpt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m# créer un dictionnaire à partir des paires clé-valeur\u001b[39;00m\n\u001b[0;32m     21\u001b[0m subset \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(subset_items)\n\u001b[1;32m---> 23\u001b[0m df \u001b[39m=\u001b[39m compute_metrics(subset, gen\u001b[39m=\u001b[39;49mgen)\n\u001b[0;32m     24\u001b[0m save_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mfigures\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mgen\u001b[39m}\u001b[39;00m\u001b[39m_correlation.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m pearson_corr \u001b[39m=\u001b[39m compute_correlations(df, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpearson\u001b[39m\u001b[39m'\u001b[39m, dataset_name\u001b[39m=\u001b[39mdataset_name, gen\u001b[39m=\u001b[39mgen, save_path\u001b[39m=\u001b[39msave_path)\n",
      "Cell \u001b[1;32mIn[24], line 72\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(dataset, gen)\u001b[0m\n\u001b[0;32m     69\u001b[0m bleu_score \u001b[39m=\u001b[39m sentence_bleu([ref_tokens], hyp_tokens)\n\u001b[0;32m     70\u001b[0m bleu_scores\u001b[39m.\u001b[39mappend(bleu_score)\n\u001b[1;32m---> 72\u001b[0m entropy_bleu_score \u001b[39m=\u001b[39m weighted_entropy_bleu\u001b[39m.\u001b[39;49mscore([ref_tokens], hyp_tokens)\n\u001b[0;32m     73\u001b[0m weighted_bleu_scores\u001b[39m.\u001b[39mappend(entropy_bleu_score)\n\u001b[0;32m     75\u001b[0m rouge_scores \u001b[39m=\u001b[39m rouge\u001b[39m.\u001b[39mget_scores(t_response_str, t_gold_response_str, avg\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\coren\\OneDrive\\Documents\\ENSAE 3A\\NLP_projet\\NLP_project\\metrics\\WBleu.py:26\u001b[0m, in \u001b[0;36mWeightedEntropyBLEU.score\u001b[1;34m(self, reference_sentences, candidate_sentence)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscore\u001b[39m(\u001b[39mself\u001b[39m, reference_sentences, candidate_sentence):\n\u001b[0;32m     25\u001b[0m     weights \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram_entropies\u001b[39m.\u001b[39mget(ngram, \u001b[39m0.0\u001b[39m) \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m ngrams(candidate_sentence, \u001b[39m1\u001b[39m)]\n\u001b[1;32m---> 26\u001b[0m     \u001b[39mreturn\u001b[39;00m sentence_bleu(reference_sentences, candidate_sentence, weights\u001b[39m=\u001b[39;49mweights, smoothing_function\u001b[39m=\u001b[39;49mSmoothingFunction()\u001b[39m.\u001b[39;49mmethod1)\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:107\u001b[0m, in \u001b[0;36msentence_bleu\u001b[1;34m(references, hypothesis, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentence_bleu\u001b[39m(\n\u001b[0;32m     21\u001b[0m     references,\n\u001b[0;32m     22\u001b[0m     hypothesis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     auto_reweigh\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m ):\n\u001b[0;32m     27\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m    Calculate BLEU score (Bilingual Evaluation Understudy) from\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39m    :rtype: float / list(float)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m corpus_bleu(\n\u001b[0;32m    108\u001b[0m         [references], [hypothesis], weights, smoothing_function, auto_reweigh\n\u001b[0;32m    109\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:210\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mfor\u001b[39;00m references, hypothesis \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(list_of_references, hypotheses):\n\u001b[0;32m    207\u001b[0m     \u001b[39m# For each order of ngram, calculate the numerator and\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[39m# denominator for the corpus-level modified precision.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_weight_length \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m--> 210\u001b[0m         p_i \u001b[39m=\u001b[39m modified_precision(references, hypothesis, i)\n\u001b[0;32m    211\u001b[0m         p_numerators[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m p_i\u001b[39m.\u001b[39mnumerator\n\u001b[0;32m    212\u001b[0m         p_denominators[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m p_i\u001b[39m.\u001b[39mdenominator\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:347\u001b[0m, in \u001b[0;36mmodified_precision\u001b[1;34m(references, hypothesis, n)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39mCalculate modified ngram precision.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39m:rtype: Fraction\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39m# Extracts all ngrams in hypothesis\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39m# Set an empty Counter if hypothesis is empty.\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m counts \u001b[39m=\u001b[39m Counter(ngrams(hypothesis, n)) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(hypothesis) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n \u001b[39melse\u001b[39;00m Counter()\n\u001b[0;32m    348\u001b[0m \u001b[39m# Extract a union of references' counts.\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[39m# max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\u001b[39;00m\n\u001b[0;32m    350\u001b[0m max_counts \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\collections\\__init__.py:597\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[0;32m    587\u001b[0m \u001b[39mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[39mof elements to their counts.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    594\u001b[0m \n\u001b[0;32m    595\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m--> 597\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(iterable, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_roc_path = r\"data/mans_data/mans_roc.json\"\n",
    "dataset_wp_path = r\"data/mans_data/mans_wp.json\"\n",
    "datasets = { 'WP': dataset_wp_path}\n",
    "\n",
    "gens = ['gpt', 'plan_write', 's2s', 'gpt_kg', 'fusion']\n",
    "\n",
    "for dataset_name, dataset_path in datasets.items():\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    \n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    for gen in gens:\n",
    "        print(f\"Generation: {gen}\")\n",
    "        \n",
    "        # on effectue des subset car le code tourne sur cpu\n",
    "        subset_size = 4\n",
    "        subset_items = list(dataset.items())[:subset_size]\n",
    "\n",
    "        # créer un dictionnaire à partir des paires clé-valeur\n",
    "        subset = dict(subset_items)\n",
    "\n",
    "        df = compute_metrics(subset, gen=gen)\n",
    "        save_path = os.path.join('figures', f\"{dataset_name}_{gen}_correlation.png\")\n",
    "        pearson_corr = compute_correlations(df, method='pearson', dataset_name=dataset_name, gen=gen, save_path=save_path)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story ID</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Human</th>\n",
       "      <th>Story</th>\n",
       "      <th>Model</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Empathy</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Engagement</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Worker ID</th>\n",
       "      <th>Assignment ID</th>\n",
       "      <th>Work time in seconds</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>When you die the afterlife is an arena where y...</td>\n",
       "      <td>3,000 years have I been fighting. Every mornin...</td>\n",
       "      <td>3,000 years have I been fighting. Every mornin...</td>\n",
       "      <td>Human</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>A2VE5IV9OD2SK1</td>\n",
       "      <td>3X87C8JFVHIT235KQ4UTS8264I6SQJ</td>\n",
       "      <td>579.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>When you die the afterlife is an arena where y...</td>\n",
       "      <td>3,000 years have I been fighting. Every mornin...</td>\n",
       "      <td>3,000 years have I been fighting. Every mornin...</td>\n",
       "      <td>Human</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A1IZ4NX41GKU4X</td>\n",
       "      <td>3DR23U6WEGL5K0SU6D4J8W9EM9LTE7</td>\n",
       "      <td>82.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>When you die the afterlife is an arena where y...</td>\n",
       "      <td>3,000 years have I been fighting. Every mornin...</td>\n",
       "      <td>3,000 years have I been fighting. Every mornin...</td>\n",
       "      <td>Human</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>A264NN7JBX4UDQ</td>\n",
       "      <td>3UJ1CZ6IZSW49HMM6C6QUX7F7UV5SA</td>\n",
       "      <td>273.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A new law is enacted that erases soldiers memo...</td>\n",
       "      <td>“Dad, you 're on TV again !” I heard Eric 's v...</td>\n",
       "      <td>“Dad, you 're on TV again !” I heard Eric 's v...</td>\n",
       "      <td>Human</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>A3CFNUD7VR2E1E</td>\n",
       "      <td>317HQ483IIZJ5SPW508YKC1EP6RINX</td>\n",
       "      <td>117.0</td>\n",
       "      <td>Eric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A new law is enacted that erases soldiers memo...</td>\n",
       "      <td>“Dad, you 're on TV again !” I heard Eric 's v...</td>\n",
       "      <td>“Dad, you 're on TV again !” I heard Eric 's v...</td>\n",
       "      <td>Human</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>A2VE5IV9OD2SK1</td>\n",
       "      <td>3T3IWE1XGHUUH3IZF4ZJ2DYS57WQTT</td>\n",
       "      <td>751.0</td>\n",
       "      <td>Eric</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Story ID                                             Prompt  \\\n",
       "0         0  When you die the afterlife is an arena where y...   \n",
       "1         0  When you die the afterlife is an arena where y...   \n",
       "2         0  When you die the afterlife is an arena where y...   \n",
       "3         1  A new law is enacted that erases soldiers memo...   \n",
       "4         1  A new law is enacted that erases soldiers memo...   \n",
       "\n",
       "                                               Human  \\\n",
       "0  3,000 years have I been fighting. Every mornin...   \n",
       "1  3,000 years have I been fighting. Every mornin...   \n",
       "2  3,000 years have I been fighting. Every mornin...   \n",
       "3  “Dad, you 're on TV again !” I heard Eric 's v...   \n",
       "4  “Dad, you 're on TV again !” I heard Eric 's v...   \n",
       "\n",
       "                                               Story  Model  Relevance  \\\n",
       "0  3,000 years have I been fighting. Every mornin...  Human          4   \n",
       "1  3,000 years have I been fighting. Every mornin...  Human          5   \n",
       "2  3,000 years have I been fighting. Every mornin...  Human          2   \n",
       "3  “Dad, you 're on TV again !” I heard Eric 's v...  Human          5   \n",
       "4  “Dad, you 're on TV again !” I heard Eric 's v...  Human          5   \n",
       "\n",
       "   Coherence  Empathy  Surprise  Engagement  Complexity       Worker ID  \\\n",
       "0          4        3         2           4           4  A2VE5IV9OD2SK1   \n",
       "1          5        1         3           4           1  A1IZ4NX41GKU4X   \n",
       "2          2        3         2           2           3  A264NN7JBX4UDQ   \n",
       "3          5        3         4           4           4  A3CFNUD7VR2E1E   \n",
       "4          4        4         4           4           4  A2VE5IV9OD2SK1   \n",
       "\n",
       "                    Assignment ID  Work time in seconds  Name  \n",
       "0  3X87C8JFVHIT235KQ4UTS8264I6SQJ                 579.0  None  \n",
       "1  3DR23U6WEGL5K0SU6D4J8W9EM9LTE7                  82.0  none  \n",
       "2  3UJ1CZ6IZSW49HMM6C6QUX7F7UV5SA                 273.0  none  \n",
       "3  317HQ483IIZJ5SPW508YKC1EP6RINX                 117.0  Eric  \n",
       "4  3T3IWE1XGHUUH3IZF4ZJ2DYS57WQTT                 751.0  Eric  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_hanna_path = r\"data\\hanna\\hanna_stories_annotations.csv\"\n",
    "data = pd.read_csv(dataset_hanna_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BaryScore Progress: 100%|██████████| 1/1 [00:02<00:00,  2.60s/it]\n",
      "Depth Score Progress: 100%|██████████| 1/1 [00:03<00:00,  3.83s/it]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "BaryScore Progress: 100%|██████████| 1/1 [00:02<00:00,  2.66s/it]\n",
      "Depth Score Progress: 100%|██████████| 1/1 [00:03<00:00,  3.81s/it]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "BaryScore Progress: 100%|██████████| 1/1 [00:02<00:00,  2.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 174\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39m# Exemple d'utilisation de la fonction\u001b[39;00m\n\u001b[0;32m    173\u001b[0m csv_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mhanna\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mhanna_stories_annotations.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 174\u001b[0m result_df \u001b[39m=\u001b[39m compute_metrics_hanna(csv_file)\n",
      "Cell \u001b[1;32mIn[33], line 109\u001b[0m, in \u001b[0;36mcompute_metrics_hanna\u001b[1;34m(csv_file)\u001b[0m\n\u001b[0;32m    107\u001b[0m bary_score \u001b[39m=\u001b[39m bary_score_metric\u001b[39m.\u001b[39mevaluate_batch([t_human_response_str], [t_story_response_str])\n\u001b[0;32m    108\u001b[0m info_lm_metric\u001b[39m.\u001b[39mprepare_idfs([t_human_response_str], [t_story_response_str])\n\u001b[1;32m--> 109\u001b[0m info_lm_score \u001b[39m=\u001b[39m info_lm_metric\u001b[39m.\u001b[39;49mevaluate_batch([t_human_response_str], [t_story_response_str])\n\u001b[0;32m    110\u001b[0m depth_score \u001b[39m=\u001b[39m depth_score_metric\u001b[39m.\u001b[39mevaluate_batch([t_human_response_str], [t_story_response_str])\n\u001b[0;32m    112\u001b[0m bary_scores_W\u001b[39m.\u001b[39mappend(bary_score[\u001b[39m'\u001b[39m\u001b[39mbaryscore_W\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\coren\\OneDrive\\Documents\\ENSAE 3A\\NLP_projet\\NLP_project\\metrics\\InfoLM.py:285\u001b[0m, in \u001b[0;36mInfoLM.evaluate_batch\u001b[1;34m(self, batch_hyps, batch_refs, idf_hyps, idf_ref)\u001b[0m\n\u001b[0;32m    283\u001b[0m     idf_ref[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    284\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 285\u001b[0m     dict_final_distribution_batch_refs, idfs_ref \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_distribution(batch_refs,\n\u001b[0;32m    286\u001b[0m                                                                          idf_ref \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_idf_weights \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    287\u001b[0m     dict_final_distribution_batch_hypothesis, idfs_hyp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_distribution(batch_hyps,\n\u001b[0;32m    288\u001b[0m                                                                                idf_hyps \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf_weights \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    289\u001b[0m mask_ref \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(batch_refs, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\coren\\OneDrive\\Documents\\ENSAE 3A\\NLP_projet\\NLP_project\\metrics\\InfoLM.py:247\u001b[0m, in \u001b[0;36mInfoLM.get_distribution\u001b[1;34m(self, tokenizer_output, idf_dic)\u001b[0m\n\u001b[0;32m    245\u001b[0m masked_input_ids[:, index_to_mask] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmask_token_id\n\u001b[0;32m    246\u001b[0m unmasked_data[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m masked_input_ids\n\u001b[1;32m--> 247\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munmasked_data, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m    248\u001b[0m logits_distribution \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m][:, index_to_mask, :]\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    249\u001b[0m dict_logits_distribution \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:666\u001b[0m, in \u001b[0;36mDistilBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[39m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[39m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    664\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m--> 666\u001b[0m dlbrt_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[0;32m    667\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    668\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    669\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    670\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    671\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    672\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    673\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    674\u001b[0m )\n\u001b[0;32m    675\u001b[0m hidden_states \u001b[39m=\u001b[39m dlbrt_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    676\u001b[0m prediction_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_transform(hidden_states)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:583\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    579\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    581\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    584\u001b[0m     x\u001b[39m=\u001b[39;49membeddings,\n\u001b[0;32m    585\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    586\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    587\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    588\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    589\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    590\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:359\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    357\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 359\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    360\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[0;32m    361\u001b[0m )\n\u001b[0;32m    362\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    364\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    296\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    297\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    298\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    299\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    300\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    301\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    302\u001b[0m )\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    304\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\coren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_metrics_hanna(csv_file):\n",
    "    dataset = pd.read_csv(csv_file)\n",
    "\n",
    "    # Supprimer les lignes où la colonne Model est égale à 'Human'\n",
    "    dataset = dataset[dataset['Model'] != 'Human']\n",
    "\n",
    "    # Supprimer les duplicats\n",
    "    dataset = dataset.drop_duplicates()\n",
    "\n",
    "    # Initialisation du corpus de référence pour la métrique WeightedEntropyBLEU\n",
    "    reference_corpus = [data['Human'] for id, data in dataset.iterrows()]\n",
    "    \n",
    "    # Initialisation des instances calculant les métriques\n",
    "    weighted_entropy_bleu = WeightedEntropyBLEU(reference_corpus)\n",
    "    rouge = Rouge()\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "    bary_score_metric = BaryScoreMetric(model_name ,use_idfs=False)\n",
    "    info_lm_metric = InfoLM(model_name, measure_to_use='kl', alpha=0.25, beta=0.25, temperature=1, use_idf_weights=False)\n",
    "    depth_score_metric = DepthScoreMetric(model_name, layers_to_consider=4)\n",
    "\n",
    "    # Initialisation des listes de stockages de calcul des métriques\n",
    "    bleu_scores = []\n",
    "    weighted_bleu_scores = []\n",
    "    rouge_1_r = []\n",
    "    rouge_1_p = []\n",
    "    rouge_1_f = []\n",
    "    rouge_2_r = []\n",
    "    rouge_2_p = []\n",
    "    rouge_2_f = []\n",
    "    rouge_l_r = []\n",
    "    rouge_l_p = []\n",
    "    rouge_l_f = []\n",
    "    bertscore_p = []\n",
    "    bertscore_r = []\n",
    "    bertscore_f1 = []\n",
    "    # moverscores = []\n",
    "    bary_scores_W = []\n",
    "    bary_scores_SD_10 = []\n",
    "    bary_scores_SD_5 = []\n",
    "    bary_scores_SD_1 = []\n",
    "    bary_scores_SD_0_5 = []\n",
    "    bary_scores_SD_0_1 = []\n",
    "    bary_scores_SD_0_01 = []\n",
    "    bary_scores_SD_0_001 = []\n",
    "    info_lm_kl = []\n",
    "    info_lm_r_kl = []\n",
    "    info_lm_sim_kl = []\n",
    "    depth_scores = []\n",
    "    relevance_scores = []\n",
    "    coherence_scores = []\n",
    "    empathy_scores = []\n",
    "    surprise_scores = []\n",
    "    engagement_scores = []\n",
    "    complexity_scores = []\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        human_response = row['Human']\n",
    "        story_response = row['Story']\n",
    "        relevance = row['Relevance']\n",
    "        coherence = row['Coherence']\n",
    "        empathy = row['Empathy']\n",
    "        surprise = row['Surprise']\n",
    "        engagement = row['Engagement']\n",
    "        complexity = row['Complexity']\n",
    "\n",
    "        relevance_scores.append(relevance)\n",
    "        coherence_scores.append(coherence)\n",
    "        empathy_scores.append(empathy)\n",
    "        surprise_scores.append(surprise)\n",
    "        engagement_scores.append(engagement)\n",
    "        complexity_scores.append(complexity)\n",
    "\n",
    "        # Tokenisation des réponses\n",
    "        t_human_response = bary_score_metric.tokenizer(human_response, return_tensors='pt')['input_ids']\n",
    "        t_story_response = bary_score_metric.tokenizer(story_response, return_tensors='pt')['input_ids']\n",
    "\n",
    "        # Convertion token IDs en strings\n",
    "        t_human_response_str = ' '.join(bary_score_metric.tokenizer.convert_ids_to_tokens(t_human_response[0]))\n",
    "        t_story_response_str = ' '.join(bary_score_metric.tokenizer.convert_ids_to_tokens(t_story_response[0]))\n",
    "\n",
    "        # Tokenisation des phrases en listes de mots pour computer BLEU\n",
    "        hyp_tokens = word_tokenize(t_story_response_str)\n",
    "        ref_tokens = word_tokenize(t_human_response_str)\n",
    "        \n",
    "        \n",
    "        # Calcul des différents scores de métriques\n",
    "        bleu_score = sentence_bleu([ref_tokens], hyp_tokens)\n",
    "        bleu_scores.append(bleu_score)\n",
    "        \n",
    "        entropy_bleu_score = weighted_entropy_bleu.score([ref_tokens], hyp_tokens)\n",
    "        weighted_bleu_scores.append(entropy_bleu_score)\n",
    "\n",
    "        rouge_scores = rouge.get_scores(t_story_response_str, t_human_response_str, avg=True)\n",
    "        rouge_1_r.append(rouge_scores['rouge-1']['r'])\n",
    "        rouge_1_p.append(rouge_scores['rouge-1']['p'])\n",
    "        rouge_1_f.append(rouge_scores['rouge-1']['f'])\n",
    "        rouge_2_r.append(rouge_scores['rouge-2']['r'])\n",
    "        rouge_2_p.append(rouge_scores['rouge-2']['p'])\n",
    "        rouge_2_f.append(rouge_scores['rouge-2']['f'])\n",
    "        rouge_l_r.append(rouge_scores['rouge-l']['r'])\n",
    "        rouge_l_p.append(rouge_scores['rouge-l']['p'])\n",
    "        rouge_l_f.append(rouge_scores['rouge-l']['f'])\n",
    "\n",
    "        \n",
    "        \n",
    "        bary_score_metric.prepare_idfs([t_human_response_str], [t_story_response_str])\n",
    "        bary_score = bary_score_metric.evaluate_batch([t_human_response_str], [t_story_response_str])\n",
    "        info_lm_metric.prepare_idfs([t_human_response_str], [t_story_response_str])\n",
    "        info_lm_score = info_lm_metric.evaluate_batch([t_human_response_str], [t_story_response_str])\n",
    "        depth_score = depth_score_metric.evaluate_batch([t_human_response_str], [t_story_response_str])\n",
    "\n",
    "        bary_scores_W.append(bary_score['baryscore_W'][0])\n",
    "        bary_scores_SD_10.append(bary_score['baryscore_SD_10'][0])\n",
    "        bary_scores_SD_5.append(bary_score['baryscore_SD_5'][0])\n",
    "        bary_scores_SD_1.append(bary_score['baryscore_SD_1'][0])\n",
    "        bary_scores_SD_0_5.append(bary_score['baryscore_SD_0.5'][0])\n",
    "        bary_scores_SD_0_1.append(bary_score['baryscore_SD_0.1'][0])\n",
    "        bary_scores_SD_0_01.append(bary_score['baryscore_SD_0.01'][0])\n",
    "        bary_scores_SD_0_001.append(bary_score['baryscore_SD_0.001'][0])\n",
    "        info_lm_kl.append(info_lm_score['kl'][0])\n",
    "        info_lm_r_kl.append(info_lm_score['r_kl'][0])\n",
    "        info_lm_sim_kl.append(info_lm_score['sim_kl'][0])\n",
    "        depth_scores.append(depth_score['depth_score'][0])\n",
    "        \n",
    "        P, R, F1 = bert_score([t_story_response_str], [t_human_response_str], lang=\"en\", model_type=model_name)\n",
    "        bertscore_p.append(P.numpy()[0])\n",
    "        bertscore_r.append(R.numpy()[0])\n",
    "        bertscore_f1.append(F1.numpy()[0])\n",
    "\n",
    "    # Créer un dictionnaire avec les scores des métriques\n",
    "    dict = {\n",
    "        'BLEU': bleu_scores,\n",
    "        'Weighted_BLEU': weighted_bleu_scores,\n",
    "        'Rouge_1_r': rouge_1_r,\n",
    "        'Rouge_1_p': rouge_1_p,\n",
    "        'Rouge_1_f': rouge_1_f,\n",
    "        'Rouge_2_r': rouge_2_r,\n",
    "        'Rouge_2_p': rouge_2_p,\n",
    "        'Rouge_2_f': rouge_2_f,\n",
    "        'Rouge_L_r': rouge_l_r,\n",
    "        'Rouge_L_p': rouge_l_p,\n",
    "        'Rouge_L_f': rouge_l_f,\n",
    "        'Bary_W': bary_scores_W,\n",
    "        'Bary_SD_10': bary_scores_SD_10,\n",
    "        'Bary_SD_5': bary_scores_SD_5,\n",
    "        'Bary_SD_1': bary_scores_SD_1,\n",
    "        'Bary_SD_0_5': bary_scores_SD_0_5,\n",
    "        'Bary_SD_0_1': bary_scores_SD_0_1,\n",
    "        'Bary_SD_0_01': bary_scores_SD_0_01,\n",
    "        'Bary_SD_0_001': bary_scores_SD_0_001,\n",
    "        'InfoLM_kl': info_lm_kl,\n",
    "        'InfoLM_r_kl': info_lm_r_kl,\n",
    "        'InfoLM_sim_kl': info_lm_sim_kl,\n",
    "        'Depth_scores': depth_scores,\n",
    "        'Bertscore_p': bertscore_p,\n",
    "        'Bertscore_r': bertscore_r,\n",
    "        'Bertscore_f1': bertscore_f1,\n",
    "        # 'moverscores': moverscores,\n",
    "        'Relevance': relevance_scores,\n",
    "        'Coherence' : coherence_scores,\n",
    "        'Empathy' : empathy_scores,\n",
    "        'Surprise' : surprise_scores,\n",
    "        'Engagement' : engagement_scores,\n",
    "        'Complexity' : complexity_scores,\n",
    "    }\n",
    "\n",
    "    # Créer un DataFrame avec les scores des métriques\n",
    "    df = pd.DataFrame(dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Exemple d'utilisation de la fonction\n",
    "csv_file = \"data\\hanna\\hanna_stories_annotations.csv\"\n",
    "result_df = compute_metrics_hanna(csv_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9654e1d1622dc0a09b1839a2873651fdc23b851a4697e83619343ed62180f16a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
